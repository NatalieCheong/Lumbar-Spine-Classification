{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "```python\n",
        "# Test Results Notebook\n",
        "\n",
        "# Import necessary libraries\n",
        "import unittest\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "from datetime import datetime\n",
        "\n",
        "# Import test modules\n",
        "from tests.test_preprocessing import TestPreprocessing\n",
        "from tests.test_models import TestModels\n",
        "from tests.test_prediction import TestPrediction\n",
        "\n",
        "class TestRunner:\n",
        "    def __init__(self):\n",
        "        self.test_results = {}\n",
        "        self.total_tests = 0\n",
        "        self.passed_tests = 0\n",
        "        self.failed_tests = 0\n",
        "        self.error_tests = 0\n",
        "\n",
        "    def run_test_suite(self, test_class, suite_name):\n",
        "        \"\"\"Run a test suite and collect results\"\"\"\n",
        "        suite = unittest.TestLoader().loadTestsFromTestCase(test_class)\n",
        "        result = unittest.TestResult()\n",
        "        suite.run(result)\n",
        "\n",
        "        # Collect results\n",
        "        suite_results = {\n",
        "            'total': suite.countTestCases(),\n",
        "            'passed': suite.countTestCases() - len(result.failures) - len(result.errors),\n",
        "            'failed': len(result.failures),\n",
        "            'errors': len(result.errors),\n",
        "            'failures': result.failures,\n",
        "            'error_details': result.errors\n",
        "        }\n",
        "\n",
        "        self.test_results[suite_name] = suite_results\n",
        "        self.total_tests += suite_results['total']\n",
        "        self.passed_tests += suite_results['passed']\n",
        "        self.failed_tests += suite_results['failed']\n",
        "        self.error_tests += suite_results['errors']\n",
        "\n",
        "        return suite_results\n",
        "\n",
        "    def display_results(self):\n",
        "        \"\"\"Display test results in a formatted way\"\"\"\n",
        "        print(\"=== Test Results Summary ===\")\n",
        "        print(f\"Total Tests: {self.total_tests}\")\n",
        "        print(f\"Passed: {self.passed_tests}\")\n",
        "        print(f\"Failed: {self.failed_tests}\")\n",
        "        print(f\"Errors: {self.error_tests}\")\n",
        "        print(\"\\nDetailed Results:\")\n",
        "\n",
        "        for suite_name, results in self.test_results.items():\n",
        "            print(f\"\\n{suite_name}:\")\n",
        "            print(f\"  Total: {results['total']}\")\n",
        "            print(f\"  Passed: {results['passed']}\")\n",
        "            print(f\"  Failed: {results['failed']}\")\n",
        "            print(f\"  Errors: {results['errors']}\")\n",
        "\n",
        "            if results['failures']:\n",
        "                print(\"\\n  Failed Tests:\")\n",
        "                for failure in results['failures']:\n",
        "                    print(f\"    - {failure[0]}: {failure[1]}\")\n",
        "\n",
        "            if results['error_details']:\n",
        "                print(\"\\n  Test Errors:\")\n",
        "                for error in results['error_details']:\n",
        "                    print(f\"    - {error[0]}: {error[1]}\")\n",
        "\n",
        "    def plot_results(self):\n",
        "        \"\"\"Plot test results\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot overall results\n",
        "        plt.subplot(1, 2, 1)\n",
        "        labels = ['Passed', 'Failed', 'Errors']\n",
        "        sizes = [self.passed_tests, self.failed_tests, self.error_tests]\n",
        "        colors = ['#2ecc71', '#e74c3c', '#f1c40f']\n",
        "        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')\n",
        "        plt.title('Overall Test Results')\n",
        "\n",
        "        # Plot results by suite\n",
        "        plt.subplot(1, 2, 2)\n",
        "        suites = list(self.test_results.keys())\n",
        "        passed = [r['passed'] for r in self.test_results.values()]\n",
        "        failed = [r['failed'] for r in self.test_results.values()]\n",
        "        errors = [r['errors'] for r in self.test_results.values()]\n",
        "\n",
        "        x = np.arange(len(suites))\n",
        "        width = 0.25\n",
        "\n",
        "        plt.bar(x - width, passed, width, label='Passed', color='#2ecc71')\n",
        "        plt.bar(x, failed, width, label='Failed', color='#e74c3c')\n",
        "        plt.bar(x + width, errors, width, label='Errors', color='#f1c40f')\n",
        "\n",
        "        plt.xlabel('Test Suites')\n",
        "        plt.ylabel('Number of Tests')\n",
        "        plt.title('Results by Test Suite')\n",
        "        plt.xticks(x, suites, rotation=45)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def analyze_test_performance():\n",
        "    \"\"\"Analyze and display test performance metrics\"\"\"\n",
        "    import time\n",
        "\n",
        "    performance_results = {}\n",
        "\n",
        "    # Test preprocessing performance\n",
        "    start_time = time.time()\n",
        "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestPreprocessing)\n",
        "    test_suite.run(unittest.TestResult())\n",
        "    performance_results['Preprocessing'] = time.time() - start_time\n",
        "\n",
        "    # Test model performance\n",
        "    start_time = time.time()\n",
        "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestModels)\n",
        "    test_suite.run(unittest.TestResult())\n",
        "    performance_results['Models'] = time.time() - start_time\n",
        "\n",
        "    # Test prediction performance\n",
        "    start_time = time.time()\n",
        "    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestPrediction)\n",
        "    test_suite.run(unittest.TestResult())\n",
        "    performance_results['Prediction'] = time.time() - start_time\n",
        "\n",
        "    # Plot performance results\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(performance_results.keys(), performance_results.values())\n",
        "    plt.title('Test Suite Performance')\n",
        "    plt.xlabel('Test Suite')\n",
        "    plt.ylabel('Execution Time (seconds)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return performance_results\n",
        "\n",
        "def generate_test_report(runner, performance_results):\n",
        "    \"\"\"Generate a comprehensive test report\"\"\"\n",
        "    report = pd.DataFrame({\n",
        "        'Test Suite': list(runner.test_results.keys()),\n",
        "        'Total Tests': [r['total'] for r in runner.test_results.values()],\n",
        "        'Passed': [r['passed'] for r in runner.test_results.values()],\n",
        "        'Failed': [r['failed'] for r in runner.test_results.values()],\n",
        "        'Errors': [r['errors'] for r in runner.test_results.values()],\n",
        "        'Execution Time': [performance_results[suite] for suite in runner.test_results.keys()]\n",
        "    })\n",
        "\n",
        "    display(report)\n",
        "\n",
        "    # Save report\n",
        "    report.to_csv('test_report.csv', index=False)\n",
        "    print(\"\\nTest report saved to 'test_report.csv'\")\n",
        "\n",
        "# Run all tests\n",
        "runner = TestRunner()\n",
        "\n",
        "print(\"Running Preprocessing Tests...\")\n",
        "runner.run_test_suite(TestPreprocessing, 'Preprocessing Tests')\n",
        "\n",
        "print(\"\\nRunning Model Tests...\")\n",
        "runner.run_test_suite(TestModels, 'Model Tests')\n",
        "\n",
        "print(\"\\nRunning Prediction Tests...\")\n",
        "runner.run_test_suite(TestPrediction, 'Prediction Tests')\n",
        "\n",
        "# Display results\n",
        "runner.display_results()\n",
        "\n",
        "# Plot results\n",
        "runner.plot_results()\n",
        "\n",
        "# Analyze performance\n",
        "performance_results = analyze_test_performance()\n",
        "print(\"\\nTest Suite Performance (seconds):\")\n",
        "for suite, time in performance_results.items():\n",
        "    print(f\"{suite}: {time:.3f}s\")\n",
        "\n",
        "# Generate report\n",
        "generate_test_report(runner, performance_results)\n",
        "```"
      ],
      "metadata": {
        "id": "swLmQmqXxHRF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}